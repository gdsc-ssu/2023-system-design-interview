# 웹 크롤러 설계

## 웹 크롤러?

![image](https://user-images.githubusercontent.com/85864699/227528267-4fdde1ee-9f14-4465-9b7d-fe99d45138a9.png)

- 로봇 또는 스파이더라고 불림
- 검색엔진 인덱싱
    - 웹페이지를 모아 검색엔진을 위한 로컬 인덱스를 만든다
    - 구글은 Googlebot이라는 웹크롤러 사용
- 웹 아카이빙
    - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모음
- 웹 마이닝
    - 인터넷에서 유용한 지식을 도출해냄.
- 웹 모니터링
    - 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링 할 수 있다.

## 좋은 웹 크롤러가 지녀야할 속성

- 규모 확장성
    - 병행성을 활용해 효과적인 웹크롤링
- 안정성
    - 잘못 작성된 HTML, 아무 반응이 없는 서버 등의 비정상적인 입력이나 환경에 잘 대응할 수 있어야 한다.
- 예절
    - 수집 대상 웹 사이트에 짧은 시간동안 너무 많은 요청을 보내선 안된다.
- 확장성
    - 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다.

## 크롤러를 설계해보자

![image](https://user-images.githubusercontent.com/85864699/227528649-2e3d3beb-92c5-4f67-9c05-52c205fb31b1.png)

1. 시작 URL 집합
- **크롤링의 시작점**
- 찾고자하는 도메인이 붙은 모든 페이지의 URL을 시작 URL로 둔다
- 전체 URL을 작은 부분집합으로 나누는 전략
- 주제별로 다른 시작 URL사용

1. 미수집 URL저장소
- **FIFO큐**
- 다운로드할 URL → 저장관리하는 컴포넌트를 미수집 URL저장소라고 함.(큐)
- 다운로드된 URL

1. HTML 다운로더
- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
- 다운로드 할 페이지의 URL은 미수집 URL저장소가 제공

1. 도메인 이름 변환기
- URL을 IP로 변환해야 웹 페이지 다운이 가능

1. 콘텐츠 파서
- 다운로드 후 파싱과 검증의 절차를 거쳐야 함
- 이상한 웹 페이지는 문제를 일으킬 수 있으며 저장공간만 낭비하기에 콘텐츠 파서를 구현해 크롤링 과정이 느려지지 않게 함.

1. 중복 콘텐츠?
- 웹사이트의 29% 가량이 중복이다.
- 데이터의 중복을 줄이고 데이터 처리에 소요되는 시간을 줄여야 한다.

1. 콘텐츠 저장소
- HTML 문서를 보관하는 시스템
- 저장할 데이터 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 한다.

1. URL 추출기
- HTML페이지를 파싱하여 링크들을 골라내는 역할을 한다

1. URL 필터
- 특정 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속시 오타가 발생하는 URL 등을 크롤링 대상에 배제하는 역할을 한다.

1. 이미 방문한 URL?
- 블룸 필터나 해시테이블을 사용해 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL 추적
- 같은 URl 여러번 처리하는 것 방지

![image](https://user-images.githubusercontent.com/85864699/227528723-550b5b41-c159-4319-8df1-ae9692e08982.png)

- 블룸필터:원소가 집합에 속하는지 여부를 검사하는데 사용되는 확률적 자료 구조

## 상세설계

1. BFS
2. 미수집 URL 저장소
3. HTML다운로더
4. 안정성 확보 전략
5. 확장성 확보 전략
6. 문제 있는 콘텐츠 감지 및 회피 전략

### DFS?BFS?

- 웹 크롤러는 보통 BFS를 사용해 탐색할 URL을 큐에 집어넣고 다른 한쪽으로 꺼낸다.
    - 문제점 1: 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다.(동일한 서버의 다른 페이지 참조)
    - 문제점 2: URL 간에 우선순위를 두지 않아 비효율적

### 미수집 URL 저장소

- 이 저장소를 잘 구현하면 URL간에 우선순위를 두지 않아 생기는 비효율적인 문제 해결 가능
- 예의를 갖춘 크롤러, URL 간의 우선순위와 신선도를 구별하는 크롤러 구현 가능

**예의**

- 웹 크롤러는 수집 대상 서버로 짧은 시간에 너무 많은 요청을 보내는 것을 삼가야함.
- 너무 많은 요청을 보낸다면 DoS 공격으로 간주하기도 함
- 예의 바른 크롤러가 되기 위해서는 한 번에 한페이지만 요청해야 한다.
- 같은 웹 사이트의 페이지를 다운 받는 테스크는 시간차를 두고 실행하면 된다.
    - 큐 라우터 : 같은 호스트에 속한 URL은 언제나 같은 큐로 간다.
    - 매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
    - FIFO큐 : 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
    - 큐 선택기 : 큐들을 순회하면서 큐에서 URL을 꺼내 해당 큐에서 나온 URL 다운로드 하도록 지정된 작업 스레드에 전달하는 역할
    - 작업 스레드 : 전달된 URL을 다운로드 하는 작업 수행

**우선순위**

- 중요한 페이지를 먼저 수집하는 것이 중요
- 페이지 랭크, 트래픽 양, 갱신빈도 등의 척도 사용

**신선도**

- 이미 다운로드한 페이지여도 주기적으로 재수집을 해야함
    - 웹 페이지의 변경 이력 활용
    - 우선순위 활용해 중요한 페이지는 좀 더 자주 재수집

미수집 URL 저장소를 위한 지속성 저장장치

- URL이 수억개에 달하기에 전부 메모리나 디스크에 보관하면 비효율적
- 대부분은 디스크에 두지만 입출력 비용을 줄이기 위해 메모리 버퍼에 큐를 둔다.

### HTML 다운로더

HTTP 프로토콜을 통해 웹페이지를 내려 받는다

- 로봇 제외 프로토콜 (Robots.txt)
    - 웹사이트가 크롤러와 소통하는 표준적인 방법
    - 웹 사이트를 긁어가기 전에 크롤러는 해당 파일에 나열된 규칙 먼저 확인
- 성능최적화
    - 분산 크롤링
        - 크롤링 작업을 여러 서버에 분산하는 방법
    - 도메인 이름 변환 결과 캐시
        - 도메인 이름 변환기는 크롤러 성능의 병목중 하나 (DNS 요청을 보내고 결과를 받는 작업이 동기적)
        - DNS 조회 결과로 얻어진 도메인 이름과 IP주소 사이의 관계를 캐시에 보관해 놓는다
    - 지역성
        - 크롤링 작업을 진행하는 서버를 지역별로 분산
        - 크롤링 서버가 크롤링 대상 서버와 지역적으로 가까우면 다운로드 시간이 줄어들 것
    - 짧은 타임아웃
        - 서버가 응답하지 않으면 크롤러는 해당 페이지 다운로드를 중단하고 다음페이지로 넘어감

- 안정성
    - 안정해시
        - 다운로더 서버들에게 부하를 분산할 때 적용 가능한 기술
        - 다운로더 서버를 쉽게 추가하고 삭제할 수 있음
    - 크롤링 상대 및 수집 데이터 저장
        - 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해두는 것이 바람직함
    - 예외처리
    - 데이터 검증

- 확장성
    - 새로운 형태의 컨텐츠를 쉽게 지원할 수 있도록 신경써야 함.
    - 예제의 경우 PNG다운로더,URL추출기, 웹 모니터 등의 확장 모듈을 둠

- 문제가 있는 콘텐츠의 감지 및 회피
    - 중복 콘텐츠
        - 해시나 체크섬
    - 거미덫
        - 크롤러를 무한 루프에 빠뜨리도록 설계한 웹페이지
        - URL의 최대 길이를 제한하면 회피 가능
    - 데이터 노이즈
        - 광고,스팸 같은 가치 없는 콘텐츠는 크롤러에게 도움이 되지 않으므로 제외되게 해야함.
